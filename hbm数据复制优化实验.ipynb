{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf56075",
   "metadata": {},
   "source": [
    "# HBM数据复制优化实验\n",
    "\n",
    "## 参与人及时间\n",
    "\n",
    "- 参与人：陈柳青，缪弘博\n",
    "- 时间：2025-08-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21377fc4",
   "metadata": {},
   "source": [
    "hbm数据复制基础代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender.py\n",
    "import ucp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "\n",
    "async def sender(ep):\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Transferring {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 创建10K个block，一起搬运（可优化为cupy array view）\n",
    "        gpu_data = cp.random.randint(0, 255, size=(NUM_BLOCKS, size), dtype=cp.uint8)\n",
    "\n",
    "        for i in range(NUM_BLOCKS):\n",
    "            await ep.send(gpu_data[i])\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "    await sender(ep)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receiver.py\n",
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Receiving {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        for i in range(NUM_BLOCKS):\n",
    "            recv_buf = cp.empty(size, dtype=cp.uint8)\n",
    "            await ep.recv(recv_buf)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85305e90",
   "metadata": {},
   "source": [
    "优化一————减少调用次数\t将多个小block拼接成1个大buffer，用offset映射回10k块。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0638a8",
   "metadata": {},
   "source": [
    "优化二————保证内存 RDMA 注册成功，走 GPU DMA。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b84f59",
   "metadata": {},
   "source": [
    "优化三————并行管道调度\t多线程发送多个通道（多连接）提高带宽利用率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c86ac",
   "metadata": {},
   "source": [
    "优化四————动态调整批量大小：根据当前的传输状态动态调整 BATCH_SIZE，在网络负载较高时降低批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd18e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次发送的数据块数量\n",
    "\n",
    "# 用于并行发送数据块的协程\n",
    "async def sender(ep, gpu_data, start_idx, end_idx):\n",
    "    for i in range(start_idx, end_idx):\n",
    "        await ep.send(gpu_data[i])\n",
    "\n",
    "async def main():\n",
    "    # 创建多个端点连接\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Transferring {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 创建NUM_BLOCKS个数据块，采用gpu内存视图优化\n",
    "        gpu_data = cp.random.randint(0, 255, size=(NUM_BLOCKS, size), dtype=cp.uint8)\n",
    "\n",
    "        # 按批次发送数据\n",
    "        tasks = []\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, NUM_BLOCKS)\n",
    "            tasks.append(sender(ep, gpu_data, i, end_idx))\n",
    "\n",
    "        # 等待所有批次完成\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次接收的数据块数量\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Receiving {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 批量接收数据\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            tasks = []\n",
    "            for j in range(i, min(i + BATCH_SIZE, NUM_BLOCKS)):\n",
    "                recv_buf = cp.empty(size, dtype=cp.uint8)\n",
    "                tasks.append(ep.recv(recv_buf))  # 异步接收\n",
    "            # 等待所有任务完成\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd9c0d",
   "metadata": {},
   "source": [
    "优化五————启用dma代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次发送的数据块数量\n",
    "\n",
    "# 用于并行发送数据块的协程\n",
    "async def sender(ep, registered_gpu_data, start_idx, end_idx, size):\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # 使用已注册内存块的 slice\n",
    "        await ep.send(registered_gpu_data[i * size:(i + 1) * size])\n",
    "\n",
    "async def main():\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Transferring {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 分配一整块 GPU 内存（连续）用于批量数据\n",
    "        raw_gpu_buffer = cp.random.randint(0, 255, size=(NUM_BLOCKS * size), dtype=cp.uint8)\n",
    "\n",
    "        # 注册 GPU 内存，启用 RDMA（重要）\n",
    "        registered_gpu_buffer = ucp.register(raw_gpu_buffer)\n",
    "\n",
    "        # 分批发送\n",
    "        tasks = []\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, NUM_BLOCKS)\n",
    "            tasks.append(sender(ep, registered_gpu_buffer, i, end_idx, size))\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次接收的数据块数量\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Receiving {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 分配整块 GPU 内存用于接收\n",
    "        raw_gpu_buffer = cp.empty(NUM_BLOCKS * size, dtype=cp.uint8)\n",
    "\n",
    "        # 注册接收缓冲区，确保走 GPU DMA\n",
    "        registered_gpu_buffer = ucp.register(raw_gpu_buffer)\n",
    "\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            tasks = []\n",
    "            for j in range(i, min(i + BATCH_SIZE, NUM_BLOCKS)):\n",
    "                offset = j * size\n",
    "                recv_buf = registered_gpu_buffer[offset:offset + size]\n",
    "                tasks.append(ep.recv(recv_buf))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089d4fd-2914-4a18-bf10-a62eb62de37c",
   "metadata": {},
   "source": [
    "优化六————  预分配最大尺寸的缓冲区并复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286044b7-4136-424d-bceb-fda642ba8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次发送的数据块数量\n",
    "\n",
    "# 最大块尺寸为2MB\n",
    "MAX_BLOCK_SIZE = 128 * (2 ** 14)  # \n",
    "MAX_BUFFER_SIZE = NUM_BLOCKS * MAX_BLOCK_SIZE  # 最大总缓冲区大小\n",
    "\n",
    "# 预分配一次最大尺寸的GPU缓冲区\n",
    "raw_gpu_buffer = cp.empty(MAX_BUFFER_SIZE, dtype=cp.uint8)\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(raw_gpu_buffer)\n",
    "\n",
    "# 用于并行发送数据块的协程\n",
    "async def sender(ep, registered_gpu_data, start_idx, end_idx, size):\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # 使用已注册内存块的 slice\n",
    "        await ep.send(registered_gpu_data[i * size:(i + 1) * size])\n",
    "\n",
    "async def main():\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Transferring {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 复用缓冲区\n",
    "        current_size = NUM_BLOCKS * size\n",
    "        raw_gpu_buffer[:current_size] = cp.random.randint(\n",
    "            0, 255, size=current_size, dtype=cp.uint8\n",
    "        )\n",
    "\n",
    "        # 分批发送\n",
    "        tasks = []\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, NUM_BLOCKS)\n",
    "            tasks.append(sender(ep, registered_gpu_buffer, i, end_idx, size))\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8dbd95-727a-4b39-b852-8cf3e69f465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次接收的数据块数量\n",
    "\n",
    "# 与发送端保持一致的最大缓冲区配置\n",
    "MAX_BLOCK_SIZE = 128 * (2 ** 14)  \n",
    "MAX_BUFFER_SIZE = NUM_BLOCKS * MAX_BLOCK_SIZE  # 最大总缓冲区大小\n",
    "\n",
    "# 预分配一次最大尺寸的GPU接收缓冲区\n",
    "raw_gpu_buffer = cp.empty(MAX_BUFFER_SIZE, dtype=cp.uint8)\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(raw_gpu_buffer)\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Receiving {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 复用预分配的缓冲区\n",
    "        current_size = NUM_BLOCKS * size\n",
    "        current_buffer = registered_gpu_buffer[:current_size]\n",
    "\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            tasks = []\n",
    "            for j in range(i, min(i + BATCH_SIZE, NUM_BLOCKS)):\n",
    "                offset = j * size\n",
    "                recv_buf = current_buffer[offset:offset + size]\n",
    "                tasks.append(ep.recv(recv_buf))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30af71-96fb-48ac-846c-8d165ff9e6ae",
   "metadata": {},
   "source": [
    "优化七————尝试使内存地址的对齐，避免UCX内部触发额外的拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4b295-bf8f-4c96-888b-31e4a99bced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次发送的数据块数量\n",
    "\n",
    "\n",
    "MAX_BLOCK_SIZE = 128 * (2 **14)  # 2MB\n",
    "ALIGNMENT = 512  # RDMA内存对齐粒度\n",
    "\n",
    "# 计算最大所需缓冲区大小\n",
    "max_buffer_size = NUM_BLOCKS * MAX_BLOCK_SIZE\n",
    "# 确保内存地址对齐\n",
    "aligned_max_size = ((max_buffer_size + ALIGNMENT - 1) // ALIGNMENT) * ALIGNMENT\n",
    "\n",
    "# 预分配一次最大尺寸的GPU缓冲区\n",
    "raw_gpu_buffer = cp.empty(aligned_max_size, dtype=cp.uint8, order='C')\n",
    "# 截取对齐后的有效区域\n",
    "aligned_buffer = raw_gpu_buffer[:max_buffer_size]\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(aligned_buffer)\n",
    "\n",
    "# 用于并行发送数据块的协程\n",
    "async def sender(ep, registered_gpu_data, start_idx, end_idx, size):\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # 使用已注册内存块的 slice\n",
    "        await ep.send(registered_gpu_data[i * size:(i + 1) * size])\n",
    "\n",
    "async def main():\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2** exp)\n",
    "        print(f\"Transferring {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 复用缓冲区\n",
    "        current_buffer_size = NUM_BLOCKS * size\n",
    "        aligned_buffer[:current_buffer_size] = cp.random.randint(\n",
    "            0, 255, size=current_buffer_size, dtype=cp.uint8\n",
    "        )\n",
    "\n",
    "        # 分批发送\n",
    "        tasks = []\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, NUM_BLOCKS)\n",
    "            tasks.append(sender(ep, registered_gpu_buffer, i, end_idx, size))\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dbe59-3051-4b4c-9b8c-40ff6741095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次接收的数据块数量\n",
    "\n",
    "# 与发送端保持一致的配置\n",
    "MAX_BLOCK_SIZE = 128 * (2 ** 14)  # 2MB\n",
    "ALIGNMENT = 512  # RDMA内存对齐粒度\n",
    "\n",
    "# 计算最大所需缓冲区大小\n",
    "max_buffer_size = NUM_BLOCKS * MAX_BLOCK_SIZE\n",
    "aligned_max_size = ((max_buffer_size + ALIGNMENT - 1) // ALIGNMENT) * ALIGNMENT\n",
    "\n",
    "# 预分配一次最大尺寸的GPU接收缓冲区\n",
    "raw_gpu_buffer = cp.empty(aligned_max_size, dtype=cp.uint8, order='C')\n",
    "# 截取对齐后的有效区域\n",
    "aligned_buffer = raw_gpu_buffer[:max_buffer_size]\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(aligned_buffer)\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"Receiving {NUM_BLOCKS} blocks of size {size} bytes\")\n",
    "\n",
    "        # 复用预分配的缓冲区\n",
    "        current_buffer_size = NUM_BLOCKS * size\n",
    "        # 本次接收使用的缓冲区\n",
    "        current_registered_buf = registered_gpu_buffer[:current_buffer_size]\n",
    "\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            tasks = []\n",
    "            for j in range(i, min(i + BATCH_SIZE, NUM_BLOCKS)):\n",
    "                offset = j * size\n",
    "                # 从复用的缓冲区中切片获取当前块的接收区域\n",
    "                recv_buf = current_registered_buf[offset:offset + size]\n",
    "                tasks.append(ep.recv(recv_buf))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8fc4a-3c2c-483d-a6f9-bc3b6b5fca09",
   "metadata": {},
   "source": [
    "添加计时用于调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97953f-315a-4274-a2eb-3c0d6f178940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "import time  \n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次发送的数据块数量\n",
    "\n",
    "\n",
    "MAX_BLOCK_SIZE = 128 * (2 **14)  # 2MB\n",
    "ALIGNMENT = 512  # RDMA内存对齐粒度\n",
    "\n",
    "# 计算最大所需缓冲区大小\n",
    "max_buffer_size = NUM_BLOCKS * MAX_BLOCK_SIZE\n",
    "# 确保内存地址对齐\n",
    "aligned_max_size = ((max_buffer_size + ALIGNMENT - 1) // ALIGNMENT) * ALIGNMENT\n",
    "\n",
    "# 预分配一次最大尺寸的GPU缓冲区\n",
    "raw_gpu_buffer = cp.empty(aligned_max_size, dtype=cp.uint8, order='C')\n",
    "# 截取对齐后的有效区域\n",
    "aligned_buffer = raw_gpu_buffer[:max_buffer_size]\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(aligned_buffer)\n",
    "\n",
    "# 用于并行发送数据块的协程\n",
    "async def sender(ep, registered_gpu_data, start_idx, end_idx, size):\n",
    "    for i in range(start_idx, end_idx):\n",
    "        await ep.send(registered_gpu_data[i * size:(i + 1) * size])\n",
    "\n",
    "async def main():\n",
    "    ep = await ucp.create_endpoint(\"receiver_ip\", 13337)\n",
    "\n",
    "    # 记录总传输任务开始时间\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    for exp in range(0, 15):  # 128B to 2MB\n",
    "        size = 128 * (2** exp)\n",
    "        print(f\"\\n===== 开始传输 {NUM_BLOCKS} 个 {size}B 块 =====\")\n",
    "\n",
    "        # 复用缓冲区\n",
    "        current_buffer_size = NUM_BLOCKS * size\n",
    "        aligned_buffer[:current_buffer_size] = cp.random.randint(\n",
    "            0, 255, size=current_buffer_size, dtype=cp.uint8\n",
    "        )\n",
    "\n",
    "        # 记录当前批次传输开始时间\n",
    "        batch_start = time.perf_counter()\n",
    "\n",
    "        # 分批发送\n",
    "        tasks = []\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, NUM_BLOCKS)\n",
    "            tasks.append(sender(ep, registered_gpu_buffer, i, end_idx, size))\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "        # 计算并打印当前批次耗时\n",
    "        batch_end = time.perf_counter()\n",
    "        batch_duration = batch_end - batch_start\n",
    "        print(f\"当前批次传输完成，耗时: {batch_duration:.4f} 秒\")\n",
    "\n",
    "    # 计算并打印总传输耗时\n",
    "    total_end = time.perf_counter()\n",
    "    total_duration = total_end - total_start\n",
    "    print(f\"\\n===== 所有传输完成，总耗时: {total_duration:.4f} 秒 =====\")\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7727f-949c-427e-9d9f-3a69fb25f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucp\n",
    "import asyncio\n",
    "import cupy as cp\n",
    "import time  \n",
    "\n",
    "NUM_BLOCKS = 10000\n",
    "BATCH_SIZE = 100  # 每次接收的数据块数量\n",
    "\n",
    "# 与发送端保持一致的配置\n",
    "MAX_BLOCK_SIZE = 128 * (2 ** 14)  # 2MB\n",
    "ALIGNMENT = 512  # RDMA内存对齐粒度\n",
    "\n",
    "# 计算最大所需缓冲区大小\n",
    "max_buffer_size = NUM_BLOCKS * MAX_BLOCK_SIZE\n",
    "aligned_max_size = ((max_buffer_size + ALIGNMENT - 1) // ALIGNMENT) * ALIGNMENT\n",
    "\n",
    "# 预分配一次最大尺寸的GPU接收缓冲区\n",
    "raw_gpu_buffer = cp.empty(aligned_max_size, dtype=cp.uint8, order='C')\n",
    "# 截取对齐后的有效区域\n",
    "aligned_buffer = raw_gpu_buffer[:max_buffer_size]\n",
    "# 仅注册一次\n",
    "registered_gpu_buffer = ucp.register(aligned_buffer)\n",
    "\n",
    "async def recv_handler(ep):\n",
    "    # 记录总接收任务开始时间\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    for exp in range(0, 15):\n",
    "        size = 128 * (2 ** exp)\n",
    "        print(f\"\\n===== 开始接收 {NUM_BLOCKS} 个 {size}B 块 =====\")\n",
    "\n",
    "        # 复用预分配的缓冲区\n",
    "        current_buffer_size = NUM_BLOCKS * size\n",
    "        current_registered_buf = registered_gpu_buffer[:current_buffer_size]\n",
    "\n",
    "        # 记录当前批次接收开始时间\n",
    "        batch_start = time.perf_counter()\n",
    "\n",
    "        # 分批接收\n",
    "        for i in range(0, NUM_BLOCKS, BATCH_SIZE):\n",
    "            tasks = []\n",
    "            for j in range(i, min(i + BATCH_SIZE, NUM_BLOCKS)):\n",
    "                offset = j * size\n",
    "                recv_buf = current_registered_buf[offset:offset + size]\n",
    "                tasks.append(ep.recv(recv_buf))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "        # 计算并打印当前批次耗时\n",
    "        batch_end = time.perf_counter()\n",
    "        batch_duration = batch_end - batch_start\n",
    "        print(f\"当前批次接收完成，耗时: {batch_duration:.4f} 秒\")\n",
    "\n",
    "    # 计算并打印总接收耗时\n",
    "    total_end = time.perf_counter()\n",
    "    total_duration = total_end - total_start\n",
    "    print(f\"\\n===== 所有接收完成，总耗时: {total_duration:.4f} 秒 =====\")\n",
    "\n",
    "    await ep.close()\n",
    "\n",
    "async def main():\n",
    "    listener = ucp.create_listener(recv_handler, port=13337)\n",
    "    while True:\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
